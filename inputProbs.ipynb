{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-16T16:32:49.829547Z",
     "start_time": "2024-03-16T16:32:47.691082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\r\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\r\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.0)\r\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (15.0.7)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a2a0f3fd9552fef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-16T16:33:02.890778Z",
     "start_time": "2024-03-16T16:33:02.867666Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m device\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "627835095c7f3272",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-16T16:53:06.960598Z",
     "start_time": "2024-03-16T16:52:40.477783Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbaaa0090964db2808ac6040f41d618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('One', -5.9982380867004395),\n",
      "  ('plus', -9.376860618591309),\n",
      "  ('one', -1.7315524816513062),\n",
      "  ('is', -1.4799405336380005),\n",
      "  ('two', -0.6048303842544556)],\n",
      " [('Good', -8.145820617675781), ('morning', -3.811469316482544)],\n",
      " [('Hello', -6.775803565979004),\n",
      "  (',', -1.4618388414382935),\n",
      "  ('how', -5.954116344451904),\n",
      "  ('are', -0.3829379677772522),\n",
      "  ('you', -0.06397750228643417),\n",
      "  ('?', -0.5431981086730957)]]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, MistralForCausalLM\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "def to_tokens_and_logprobs(model, tokenizer, input_texts):\n",
    "    input_ids = tokenizer(input_texts, padding=True, return_tensors=\"pt\").input_ids\n",
    "    outputs = model(input_ids)\n",
    "    probs = torch.log_softmax(outputs.logits, dim=-1).detach()\n",
    "\n",
    "    # collect the probability of the generated token -- probability at index 0 corresponds to the token at index 1\n",
    "    probs = probs[:, :-1, :]\n",
    "    input_ids = input_ids[:, 1:]\n",
    "    gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)\n",
    "\n",
    "    batch = []\n",
    "    for input_sentence, input_probs in zip(input_ids, gen_probs):\n",
    "        text_sequence = []\n",
    "        for token, p in zip(input_sentence, input_probs):\n",
    "            if token not in tokenizer.all_special_ids:\n",
    "                text_sequence.append((tokenizer.decode(token), p.item()))\n",
    "        batch.append(text_sequence)\n",
    "    return batch\n",
    "\n",
    "model = MistralForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "input_texts = [\"One plus one is two\", \"Good morning\", \"Hello, how are you?\"]\n",
    "\n",
    "batch = to_tokens_and_logprobs(model, tokenizer, input_texts)\n",
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1548ed90d0af371",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-16T16:59:43.744700Z",
     "start_time": "2024-03-16T16:59:32.390008Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install ctransformers[cuda]>=0.2.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1503e900fc93444",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-16T17:09:44.922105Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-7B-GGUF\", model_file=\"llama-2-7b.Q4_K_M.gguf\", model_type=\"llama\", gpu_layers=0)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "input_texts = [\"One plus one is two\", \"Good morning\", \"Hello, how are you?\"]\n",
    "\n",
    "batch = to_tokens_and_logprobs(model, tokenizer, input_texts)\n",
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adc5ee37afc0821f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-16T17:09:35.700888Z",
     "start_time": "2024-03-16T17:09:35.697007Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
