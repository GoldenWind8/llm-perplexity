{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c699404c9e91e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb8f343a5c67823b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-16T20:58:22.671449Z",
     "start_time": "2024-03-16T20:58:19.990577Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text: what is your favorite colour?\n",
      "\n",
      "I love the colour of the sun. I love the\n",
      "[[('what', -9.269655227661133),\n",
      "  (' is', -1.9438844919204712),\n",
      "  (' your', -3.602717876434326),\n",
      "  (' favorite', -1.775857925415039),\n",
      "  (' colour', -6.8239946365356445)],\n",
      " [('One', -5.882714748382568),\n",
      "  (' plus', -9.785088539123535),\n",
      "  (' one', -0.7229113578796387),\n",
      "  (' is', -2.4940872192382812),\n",
      "  (' two', -6.137444496154785)],\n",
      " [('Good', -7.579043865203857), (' morning', -1.826709270477295)],\n",
      " [(',', -2.3431499004364014),\n",
      "  (' how', -4.3397216796875),\n",
      "  (' are', -2.6824662685394287),\n",
      "  (' you', -0.41092735528945923),\n",
      "  ('?', -1.8950951099395752)],\n",
      " [('what', -9.269655227661133),\n",
      "  (' is', -1.9438844919204712),\n",
      "  (' your', -3.602717876434326),\n",
      "  (' favorite', -1.775857925415039),\n",
      "  (' game', -3.8326237201690674)]]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "def to_tokens_and_logprobs(model, tokenizer, input_texts):\n",
    "    input_ids = tokenizer(input_texts, padding=True, return_tensors=\"pt\").input_ids\n",
    "    outputs = model(input_ids)\n",
    "    probs = torch.log_softmax(outputs.logits, dim=-1).detach()\n",
    "\n",
    "    outputs = model.generate(input_ids)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Output text:\", decoded_output)\n",
    "\n",
    "    # collect the probability of the generated token -- probability at index 0 corresponds to the token at index 1\n",
    "    probs = probs[:, :-1, :]\n",
    "    input_ids = input_ids[:, 1:]\n",
    "    gen_probs = torch.gather(probs, 2, input_ids[:, :, None]).squeeze(-1)\n",
    "\n",
    "    batch = []\n",
    "    for input_sentence, input_probs in zip(input_ids, gen_probs):\n",
    "        text_sequence = []\n",
    "        for token, p in zip(input_sentence, input_probs):\n",
    "            if token not in tokenizer.all_special_ids:\n",
    "                text_sequence.append((tokenizer.decode(token), p.item()))\n",
    "        batch.append(text_sequence)\n",
    "    return batch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "input_texts = [\"what is your favorite colour\",\"One plus one is two\", \"Good morning\", \"Hello, how are you?\", \"what is your favorite game\"]\n",
    "\n",
    "batch = to_tokens_and_logprobs(model, tokenizer, input_texts)\n",
    "pprint(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b047a84be0073bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-16T20:56:34.775829Z",
     "start_time": "2024-03-16T20:56:32.995611Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: One plus one is two\n",
      "Predicted output: \n",
      " of one. that.\n",
      "Input text: Good morning\n",
      "Predicted output: \n",
      "TheTheThe news,\n",
      "Input text: Hello, how are you?\n",
      "Predicted output: , I about you doing\n",
      "\n",
      "Input text: what is your favorite \n",
      "Predicted output: \n",
      " is the favorite partÂ \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "def to_tokens_and_logprobs(model, tokenizer, input_texts):\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer(input_texts, padding=True, return_tensors=\"pt\").input_ids\n",
    "    # Get model outputs\n",
    "    outputs = model(input_ids)\n",
    "    # Calculate log probabilities\n",
    "    probs = torch.log_softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Convert logits to probabilities for easier interpretation\n",
    "    probs = probs.exp().detach()\n",
    "\n",
    "    # Get the most probable token IDs for each position in the input text\n",
    "    predicted_ids = torch.argmax(probs, dim=-1)\n",
    "\n",
    "    # Decode the token IDs to text\n",
    "    for i, input_id in enumerate(input_ids):\n",
    "        output_text = tokenizer.decode(predicted_ids[i], skip_special_tokens=True)\n",
    "        print(f\"Input text: {input_texts[i]}\")\n",
    "        print(f\"Predicted output: {output_text}\")\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "# Example input text\n",
    "input_texts = [\"One plus one is two\", \"Good morning\", \"Hello, how are you?\", \"what is your favorite \"]\n",
    "\n",
    "# Generate and print output text\n",
    "to_tokens_and_logprobs(model, tokenizer, input_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5c12248115f29d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-16T21:50:32.157602Z",
     "start_time": "2024-03-16T21:50:27.878527Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "mistral is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:286\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 286\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\requests\\models.py:1021\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1020\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[1;32m-> 1021\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mistral/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mRepositoryNotFoundError\u001B[0m                   Traceback (most recent call last)",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\transformers\\utils\\hub.py:385\u001B[0m, in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    383\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    384\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[1;32m--> 385\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    387\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    388\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    389\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    390\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    391\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    392\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    393\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    394\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    395\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    396\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    397\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    398\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    116\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 118\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\huggingface_hub\\file_download.py:1368\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001B[0m\n\u001B[0;32m   1366\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, RepositoryNotFoundError) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(head_call_error, GatedRepoError):\n\u001B[0;32m   1367\u001B[0m     \u001B[38;5;66;03m# Repo not found => let's raise the actual error\u001B[39;00m\n\u001B[1;32m-> 1368\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m head_call_error\n\u001B[0;32m   1369\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1370\u001B[0m     \u001B[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\huggingface_hub\\file_download.py:1238\u001B[0m, in \u001B[0;36mhf_hub_download\u001B[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001B[0m\n\u001B[0;32m   1237\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1238\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m \u001B[43mget_hf_file_metadata\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1239\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1240\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43metag_timeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1243\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlibrary_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlibrary_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1244\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlibrary_version\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlibrary_version\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1245\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1246\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1247\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EntryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m http_error:\n\u001B[0;32m   1248\u001B[0m     \u001B[38;5;66;03m# Cache the non-existence of the file and raise\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    116\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m smoothly_deprecate_use_auth_token(fn_name\u001B[38;5;241m=\u001B[39mfn\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, has_token\u001B[38;5;241m=\u001B[39mhas_token, kwargs\u001B[38;5;241m=\u001B[39mkwargs)\n\u001B[1;32m--> 118\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\huggingface_hub\\file_download.py:1631\u001B[0m, in \u001B[0;36mget_hf_file_metadata\u001B[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001B[0m\n\u001B[0;32m   1630\u001B[0m \u001B[38;5;66;03m# Retrieve metadata\u001B[39;00m\n\u001B[1;32m-> 1631\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1632\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHEAD\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1633\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1634\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1635\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1636\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1637\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1638\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1639\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1640\u001B[0m hf_raise_for_status(r)\n",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\huggingface_hub\\file_download.py:385\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[1;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[0;32m    384\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m follow_relative_redirects:\n\u001B[1;32m--> 385\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43m_request_wrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    387\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    388\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_relative_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    389\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    392\u001B[0m     \u001B[38;5;66;03m# If redirection, we redirect only relative paths.\u001B[39;00m\n\u001B[0;32m    393\u001B[0m     \u001B[38;5;66;03m# This is useful in case of a renamed repository.\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\huggingface_hub\\file_download.py:409\u001B[0m, in \u001B[0;36m_request_wrapper\u001B[1;34m(method, url, follow_relative_redirects, **params)\u001B[0m\n\u001B[0;32m    408\u001B[0m response \u001B[38;5;241m=\u001B[39m get_session()\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams)\n\u001B[1;32m--> 409\u001B[0m \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    410\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:323\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    315\u001B[0m     message \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    316\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse\u001B[38;5;241m.\u001B[39mstatus_code\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Client Error.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    317\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m make sure you are authenticated.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n\u001B[1;32m--> 323\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m RepositoryNotFoundError(message, response) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m400\u001B[39m:\n",
      "\u001B[1;31mRepositoryNotFoundError\u001B[0m: 401 Client Error. (Request ID: Root=1-65f61426-5b9f119f21b2133d7ec362e3;0ddc73bf-683d-4654-9ed7-6554e065ffdd)\n\nRepository Not Found for url: https://huggingface.co/mistral/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[35], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m perplexity \u001B[38;5;241m=\u001B[39m load(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mperplexity\u001B[39m\u001B[38;5;124m\"\u001B[39m, module_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetric\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m input_texts \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlorem ipsum\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHappy Birthday!\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBienvenue\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m----> 5\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mperplexity\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmistral\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m                             \u001B[49m\u001B[43madd_start_token\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m                             \u001B[49m\u001B[43mpredictions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_texts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m pprint(results)\n",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\evaluate\\module.py:462\u001B[0m, in \u001B[0;36mEvaluationModule.compute\u001B[1;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[0;32m    460\u001B[0m inputs \u001B[38;5;241m=\u001B[39m {input_name: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[input_name] \u001B[38;5;28;01mfor\u001B[39;00m input_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feature_names()}\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m temp_seed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseed):\n\u001B[1;32m--> 462\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcompute_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    464\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_writer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    465\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_writer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--perplexity\\8ab643ad86f568b7d1d5f7822373fa7401ff5ff0297ccf114b0ca6a33be96bc0\\perplexity.py:114\u001B[0m, in \u001B[0;36mPerplexity._compute\u001B[1;34m(self, predictions, model_id, batch_size, add_start_token, device, max_length)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    112\u001B[0m     device \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 114\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_id\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    115\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    117\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m AutoTokenizer\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_id)\n",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:488\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m commit_hash \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    486\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(config, PretrainedConfig):\n\u001B[0;32m    487\u001B[0m         \u001B[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001B[39;00m\n\u001B[1;32m--> 488\u001B[0m         resolved_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m            \u001B[49m\u001B[43mCONFIG_NAME\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[43m            \u001B[49m\u001B[43m_raise_exceptions_for_missing_entries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m            \u001B[49m\u001B[43m_raise_exceptions_for_connection_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    495\u001B[0m         commit_hash \u001B[38;5;241m=\u001B[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001B[0;32m    496\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\myfirst\\lib\\site-packages\\transformers\\utils\\hub.py:406\u001B[0m, in \u001B[0;36mcached_file\u001B[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[0;32m    400\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[0;32m    401\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to access a gated repo.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mMake sure to request access at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    402\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and pass a token having permission to this repo either \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    403\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    404\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    405\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RepositoryNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 406\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[0;32m    407\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a local folder and is not a valid model identifier \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    408\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlisted on \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/models\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mIf this is a private repository, make sure to pass a token \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    409\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    410\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`token=<your_token>`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    411\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RevisionNotFoundError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    413\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[0;32m    414\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrevision\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    415\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfor this model name. Check the model page at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    416\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://huggingface.co/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for available revisions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    417\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mOSError\u001B[0m: mistral is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "perplexity = load(\"perplexity\", module_type=\"metric\")\n",
    "input_texts = [\"lorem ipsum\", \"Happy Birthday!\", \"Bienvenue\"]\n",
    "\n",
    "results = perplexity.compute(model_id='mistral',\n",
    "                             add_start_token=False,\n",
    "                             predictions=input_texts)\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7ceb71a4ac583ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-16T21:45:31.615462Z",
     "start_time": "2024-03-16T21:45:13.312105Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting datasets>=2.0.0 (from evaluate)\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from evaluate) (1.24.3)\n",
      "Collecting dill (from evaluate)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from evaluate) (4.65.0)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Downloading multiprocess-0.70.16-py38-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from evaluate) (0.20.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from evaluate) (23.1)\n",
      "Collecting responses<0.19 (from evaluate)\n",
      "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Collecting pyarrow>=12.0.0 (from datasets>=2.0.0->evaluate)\n",
      "  Downloading pyarrow-15.0.1-cp38-cp38-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets>=2.0.0->evaluate)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting aiohttp (from datasets>=2.0.0->evaluate)\n",
      "  Downloading aiohttp-3.9.3-cp38-cp38-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from pandas->evaluate) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Downloading frozenlist-1.4.1-cp38-cp38-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Downloading multidict-6.0.5-cp38-cp38-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Downloading yarl-1.9.4-cp38-cp38-win_amd64.whl.metadata (32 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sashi\\.conda\\envs\\myfirst\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 84.1/84.1 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "   ---------------------------------------- 0.0/510.5 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 327.7/510.5 kB 6.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 419.8/510.5 kB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 510.5/510.5 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "   ---------------------------------------- 0.0/132.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 132.6/132.6 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.4.1-cp38-cp38-win_amd64.whl (29 kB)\n",
      "Downloading aiohttp-3.9.3-cp38-cp38-win_amd64.whl (367 kB)\n",
      "   ---------------------------------------- 0.0/367.6 kB ? eta -:--:--\n",
      "   --------------------------------- ------ 307.2/367.6 kB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 367.6/367.6 kB 7.6 MB/s eta 0:00:00\n",
      "Downloading pyarrow-15.0.1-cp38-cp38-win_amd64.whl (24.9 MB)\n",
      "   ---------------------------------------- 0.0/24.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.4/24.9 MB 8.5 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.5/24.9 MB 7.3 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.6/24.9 MB 4.4 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 1.0/24.9 MB 5.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.4/24.9 MB 5.8 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 1.8/24.9 MB 6.2 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.2/24.9 MB 6.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.6/24.9 MB 7.0 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 3.0/24.9 MB 7.2 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.4/24.9 MB 7.2 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 3.8/24.9 MB 7.4 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 4.2/24.9 MB 7.4 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.6/24.9 MB 7.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.9/24.9 MB 7.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.1/24.9 MB 7.3 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.2/24.9 MB 6.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 5.5/24.9 MB 6.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 5.9/24.9 MB 7.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.2/24.9 MB 7.0 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.6/24.9 MB 7.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 6.9/24.9 MB 7.0 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.3/24.9 MB 7.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 7.6/24.9 MB 7.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.1/24.9 MB 7.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.4/24.9 MB 7.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.7/24.9 MB 7.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.0/24.9 MB 6.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.2/24.9 MB 6.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.6/24.9 MB 6.8 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.7/24.9 MB 6.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.0/24.9 MB 6.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 10.3/24.9 MB 6.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.6/24.9 MB 6.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.9/24.9 MB 6.8 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.2/24.9 MB 6.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.5/24.9 MB 6.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 11.8/24.9 MB 6.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 12.2/24.9 MB 6.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.6/24.9 MB 6.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.9/24.9 MB 6.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.3/24.9 MB 6.6 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.6/24.9 MB 6.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 13.8/24.9 MB 6.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.2/24.9 MB 6.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.4/24.9 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 14.7/24.9 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.0/24.9 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.5/24.9 MB 6.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.9/24.9 MB 6.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.3/24.9 MB 6.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 16.6/24.9 MB 6.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 16.8/24.9 MB 6.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 17.3/24.9 MB 6.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.4/24.9 MB 6.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 17.8/24.9 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.1/24.9 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.3/24.9 MB 6.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.5/24.9 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.7/24.9 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.7/24.9 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.9/24.9 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 19.1/24.9 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.3/24.9 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.6/24.9 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.7/24.9 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 19.9/24.9 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.1/24.9 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.3/24.9 MB 5.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.4/24.9 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.7/24.9 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.9/24.9 MB 5.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.1/24.9 MB 5.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.1/24.9 MB 5.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.2/24.9 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.4/24.9 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.6/24.9 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.9/24.9 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.1/24.9 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.3/24.9 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.5/24.9 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.7/24.9 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.8/24.9 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.0/24.9 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.2/24.9 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.4/24.9 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.5/24.9 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.6/24.9 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.8/24.9 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.0/24.9 MB 4.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.2/24.9 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.9 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.9 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.9 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.7/24.9 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.9 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.9 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/24.9 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.9/24.9 MB 4.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp38-cp38-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.8/50.8 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp38-cp38-win_amd64.whl (28 kB)\n",
      "Downloading yarl-1.9.4-cp38-cp38-win_amd64.whl (77 kB)\n",
      "   ---------------------------------------- 0.0/77.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 77.1/77.1 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets, evaluate\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 frozenlist-1.4.1 multidict-6.0.5 multiprocess-0.70.16 pyarrow-15.0.1 pyarrow-hotfix-0.6 responses-0.18.0 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install -U evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47e0d42a5c41fb77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-16T22:00:13.350974Z",
     "start_time": "2024-03-16T22:00:11.625738Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 24.39\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # Replace with the desired model name or path\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the input text X and the corresponding output text Y\n",
    "input_text = \"What is the capital of France?\"\n",
    "output_text = \"The capital of France is Paris.\"\n",
    "\n",
    "# Tokenize the input and output texts\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "output_ids = tokenizer.encode(output_text, return_tensors=\"pt\")\n",
    "\n",
    "# Concatenate the input and output token IDs\n",
    "input_output_ids = torch.cat((input_ids, output_ids), dim=-1)\n",
    "# Create attention mask\n",
    "attention_mask = torch.ones_like(input_output_ids)\n",
    "\n",
    "# Compute the perplexity\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_output_ids, attention_mask=attention_mask, labels=input_output_ids)\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "\n",
    "print(f\"Perplexity: {perplexity.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd5488001313044",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"  # Replace with the desired model name or path\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the input text X and the corresponding output text Y\n",
    "input_text = \"What is the capital of France?\"\n",
    "output_text = \"The capital of France is Paris.\"\n",
    "\n",
    "# Tokenize the input and output texts\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "output_ids = tokenizer.encode(output_text, return_tensors=\"pt\")\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = torch.ones_like(output_ids)\n",
    "\n",
    "# Compute the perplexity\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, labels=output_ids, attention_mask=attention_mask)\n",
    "    loss = outputs.loss\n",
    "    perplexity = torch.exp(loss)\n",
    "\n",
    "print(f\"Perplexity: {perplexity.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aadd995290f83a97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-17T10:50:50.960007Z",
     "start_time": "2024-03-17T10:50:50.948009Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 235]\n"
     ]
    }
   ],
   "source": [
    "print([i+1 for i in [1,2,3,234]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f50f45d3bbd65777",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T22:34:07.986826Z",
     "start_time": "2024-03-18T22:34:07.974240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0, 1, 2, 3, 4, 5, 6]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([x for x in range(10)])\n",
    "y = [x for x in range(10)]\n",
    "y[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b29e033cdd58bcea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
